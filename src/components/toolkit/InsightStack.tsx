'use client';

// FRESCO Insight Stackâ„¢ - With Voice & File Upload Support

import { useState, useEffect, useCallback, useRef } from 'react';
import { motion, AnimatePresence } from 'framer-motion';
import { 
  ChevronLeft, 
  MoreHorizontal, 
  Download, 
  Plus, 
  Trash2, 
  Sparkles,
  Loader2,
  RefreshCw,
  FileText,
  Copy,
  Check,
  Mail,
  FileDown,
  X,
  Mic,
  MicOff,
  Upload,
  File,
  Image,
  FileType
} from 'lucide-react';
import { cn, debounce, formatRelativeTime } from '@/lib/utils';
import { useFrescoStore } from '@/lib/store';
import { TOOLKITS, type ThinkingModeId } from '@/types';
import { ThinkingLensSelector } from '@/components/ui/ThinkingLensSelector';
import { SentenceOfTruth } from '@/components/ui/SentenceOfTruth';

interface InsightStackProps {
  sessionId: string;
  workspaceId: string;
  onBack?: () => void;
}

interface UploadedFile {
  name: string;
  type: string;
  content: string; // base64 or extracted text
  size: number;
}

export function InsightStack({ sessionId, workspaceId, onBack }: InsightStackProps) {
  const {
    sessions,
    workspaces,
    updateSessionStep,
    setSessionLens,
    setSentenceOfTruth,
    toggleSentenceLock,
    addInsight,
  } = useFrescoStore();
  
  const session = sessions.find((s) => s.id === sessionId);
  const workspace = workspaces.find((w) => w.id === workspaceId);
  const toolkit = session ? TOOLKITS[session.toolkitType] : null;
  
  const [stepResponses, setStepResponses] = useState<Record<number, string>>({});
  const [stepFiles, setStepFiles] = useState<Record<number, UploadedFile[]>>({});
  const [isGenerating, setIsGenerating] = useState(false);
  const [manualInsights, setManualInsights] = useState<string[]>([]);
  const [manualSentence, setManualSentence] = useState('');
  const [aiContent, setAiContent] = useState<{ insights: string[]; sentenceOfTruth: string; necessaryMoves: string[] }>({ insights: [], sentenceOfTruth: '', necessaryMoves: [] });
  const [showExportModal, setShowExportModal] = useState(false);
  const [exportStatus, setExportStatus] = useState<string | null>(null);
  const [emailAddress, setEmailAddress] = useState('');
  
  // Voice recording state
  const [isRecording, setIsRecording] = useState<number | null>(null);
  const [recordingTime, setRecordingTime] = useState(0);
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const audioChunksRef = useRef<Blob[]>([]);
  const recordingTimerRef = useRef<NodeJS.Timeout | null>(null);
  
  // File input refs
  const fileInputRefs = useRef<Record<number, HTMLInputElement | null>>({});
  
  // Load existing session data
  useEffect(() => {
    if (session?.steps) {
      const responses: Record<number, string> = {};
      session.steps.forEach((step) => { responses[step.stepNumber] = step.response || ''; });
      setStepResponses(responses);
    }
    if (session?.sentenceOfTruth?.content) {
      setManualSentence(session.sentenceOfTruth.content);
    }
  }, [session?.id]);
  
  const generateContent = useCallback(async () => {
    if (isGenerating) return;
    setIsGenerating(true);
    try {
      const response = await fetch('/api/generate', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          context: stepResponses[1] || '',
          observations: stepResponses[2] || '',
          patterns: stepResponses[3] || '',
          tensions: stepResponses[4] || '',
          insightExtraction: stepResponses[5] || '',
          thinkingLens: session?.thinkingLens || 'automatic',
          // Include file info if any
          attachedFiles: Object.entries(stepFiles).flatMap(([step, files]) => 
            files.map(f => ({ step: Number(step), name: f.name, type: f.type }))
          ),
        }),
      });
      if (response.ok) {
        const data = await response.json();
        setAiContent(data);
        if (data.sentenceOfTruth && !manualSentence) {
          setManualSentence(data.sentenceOfTruth);
          setSentenceOfTruth(sessionId, data.sentenceOfTruth);
        }
      }
    } catch (e) { console.error('Failed to generate:', e); }
    setIsGenerating(false);
  }, [stepResponses, stepFiles, session?.thinkingLens, sessionId, manualSentence, setSentenceOfTruth, isGenerating]);
  
  const debouncedSave = useCallback(
    debounce((stepNumber: number, value: string) => {
      updateSessionStep(sessionId, stepNumber, value);
    }, 500),
    [sessionId, updateSessionStep]
  );
  
  // Track if we've already auto-generated for current content
  const [hasAutoGenerated, setHasAutoGenerated] = useState(false);
  
  // Auto-generate only once when there's enough content (not on every change)
  useEffect(() => {
    const hasContent = Object.values(stepResponses).some((v) => v.trim().length > 50);
    if (hasContent && !isGenerating && !hasAutoGenerated && aiContent.insights.length === 0) {
      const timeout = setTimeout(() => {
        generateContent();
        setHasAutoGenerated(true);
      }, 3000);
      return () => clearTimeout(timeout);
    }
  }, [stepResponses, isGenerating, hasAutoGenerated, aiContent.insights.length]);
  
  const handleStepChange = (stepNumber: number, value: string) => {
    setStepResponses((prev) => ({ ...prev, [stepNumber]: value }));
    debouncedSave(stepNumber, value);
    // Reset auto-generate flag when user makes significant changes
    if (value.length > 50) {
      setHasAutoGenerated(false);
    }
  };
  
  const handleLensChange = (lens: ThinkingModeId) => {
    setSessionLens(sessionId, lens);
    setTimeout(() => generateContent(), 500);
  };
  
  // Voice Recording Functions
  const startRecording = async (stepNumber: number) => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      const mediaRecorder = new MediaRecorder(stream);
      mediaRecorderRef.current = mediaRecorder;
      audioChunksRef.current = [];
      
      mediaRecorder.ondataavailable = (event) => {
        audioChunksRef.current.push(event.data);
      };
      
      mediaRecorder.onstop = async () => {
        const audioBlob = new Blob(audioChunksRef.current, { type: 'audio/webm' });
        stream.getTracks().forEach(track => track.stop());
        
        // Convert to base64 and send for transcription
        const reader = new FileReader();
        reader.onloadend = async () => {
          const base64Audio = reader.result as string;
          await transcribeAudio(stepNumber, base64Audio);
        };
        reader.readAsDataURL(audioBlob);
      };
      
      mediaRecorder.start();
      setIsRecording(stepNumber);
      setRecordingTime(0);
      
      // Start timer
      recordingTimerRef.current = setInterval(() => {
        setRecordingTime(t => t + 1);
      }, 1000);
      
    } catch (error) {
      console.error('Failed to start recording:', error);
      alert('Could not access microphone. Please check permissions.');
    }
  };
  
  const stopRecording = () => {
    if (mediaRecorderRef.current && isRecording !== null) {
      mediaRecorderRef.current.stop();
      setIsRecording(null);
      if (recordingTimerRef.current) {
        clearInterval(recordingTimerRef.current);
      }
    }
  };
  
  const transcribeAudio = async (stepNumber: number, base64Audio: string) => {
    // For now, add a placeholder - in production, this would call a transcription API
    // You could use OpenAI Whisper, Deepgram, or similar
    try {
      const response = await fetch('/api/transcribe', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ audio: base64Audio }),
      });
      
      if (response.ok) {
        const { text } = await response.json();
        const currentText = stepResponses[stepNumber] || '';
        const newText = currentText ? `${currentText}\n\n${text}` : text;
        handleStepChange(stepNumber, newText);
      } else {
        // Fallback message if transcription API not set up
        const currentText = stepResponses[stepNumber] || '';
        handleStepChange(stepNumber, currentText + '\n\n[Voice recording captured - transcription requires API setup]');
      }
    } catch (error) {
      console.error('Transcription failed:', error);
      const currentText = stepResponses[stepNumber] || '';
      handleStepChange(stepNumber, currentText + '\n\n[Voice note recorded]');
    }
  };
  
  // File Upload Functions
  const handleFileUpload = async (stepNumber: number, files: FileList | null) => {
    if (!files || files.length === 0) return;
    
    const newFiles: UploadedFile[] = [];
    
    for (const file of Array.from(files)) {
      const reader = new FileReader();
      
      await new Promise<void>((resolve) => {
        reader.onload = async (e) => {
          const content = e.target?.result as string;
          
          newFiles.push({
            name: file.name,
            type: file.type,
            content: content,
            size: file.size,
          });
          
          // Extract text from file and add to step
          const extractedText = await extractFileContent(file, content);
          if (extractedText) {
            const currentText = stepResponses[stepNumber] || '';
            const newText = currentText 
              ? `${currentText}\n\n--- From ${file.name} ---\n${extractedText}`
              : `--- From ${file.name} ---\n${extractedText}`;
            handleStepChange(stepNumber, newText);
          }
          
          resolve();
        };
        
        if (file.type.startsWith('image/')) {
          reader.readAsDataURL(file);
        } else {
          reader.readAsText(file);
        }
      });
    }
    
    setStepFiles(prev => ({
      ...prev,
      [stepNumber]: [...(prev[stepNumber] || []), ...newFiles]
    }));
  };
  
  const extractFileContent = async (file: File, content: string): Promise<string> => {
    // Handle different file types
    if (file.type === 'text/plain' || file.type === 'text/markdown') {
      return content;
    }
    
    if (file.type === 'application/json') {
      try {
        const json = JSON.parse(content);
        return JSON.stringify(json, null, 2);
      } catch {
        return content;
      }
    }
    
    if (file.type.startsWith('image/')) {
      // For images, we'd send to Claude for analysis in production
      return `[Image uploaded: ${file.name}]`;
    }
    
    if (file.type === 'application/pdf') {
      return `[PDF uploaded: ${file.name} - content extraction requires additional setup]`;
    }
    
    // CSV files
    if (file.type === 'text/csv' || file.name.endsWith('.csv')) {
      return content;
    }
    
    return `[File uploaded: ${file.name}]`;
  };
  
  const removeFile = (stepNumber: number, fileName: string) => {
    setStepFiles(prev => ({
      ...prev,
      [stepNumber]: (prev[stepNumber] || []).filter(f => f.name !== fileName)
    }));
  };
  
  const formatRecordingTime = (seconds: number) => {
    const mins = Math.floor(seconds / 60);
    const secs = seconds % 60;
    return `${mins}:${secs.toString().padStart(2, '0')}`;
  };
  
  const handleAddManualInsight = () => setManualInsights((prev) => [...prev, '']);
  const handleManualInsightChange = (index: number, value: string) => {
    setManualInsights((prev) => { const u = [...prev]; u[index] = value; return u; });
  };
  const handleRemoveManualInsight = (index: number) => {
    setManualInsights((prev) => prev.filter((_, i) => i !== index));
  };
  const handleSentenceChange = (content: string) => {
    setManualSentence(content);
    setSentenceOfTruth(sessionId, content);
  };
  const handleToggleLock = () => toggleSentenceLock(sessionId);
  const handleUseSuggestion = (suggestion: string) => {
    setManualSentence(suggestion);
    setSentenceOfTruth(sessionId, suggestion);
  };
  
  const getLensHint = (step: typeof toolkit.steps[0]) => {
    const lens = session?.thinkingLens || 'automatic';
    if (lens === 'automatic') return null;
    const hints: Record<string, Record<number, string>> = {
      critical: { 
        1: 'Question the framing of this context', 
        2: 'What evidence supports each observation?', 
        3: 'Are these patterns real or assumed?', 
        4: 'What would disprove your hypothesis?', 
        5: 'Test this insight against contrary evidence' 
      },
      systems: { 
        1: 'What system is this part of?', 
        2: 'Look for feedback loops and dependencies', 
        3: 'How do these patterns connect?', 
        4: 'Where might the system be fighting itself?', 
        5: 'What emerges from these interactions?' 
      },
      design: { 
        1: 'Who are the humans in this context?', 
        2: 'What emotions are present?', 
        3: 'What needs are being met or unmet?', 
        4: 'Where is friction in the experience?', 
        5: 'How might this feel to those involved?' 
      },
      product: { 
        1: 'What is the product/service context?', 
        2: 'What user behaviors are you seeing?', 
        3: 'What patterns suggest product-market fit or gaps?', 
        4: 'Where do user needs conflict with business goals?', 
        5: 'What is the core value proposition emerging?' 
      },
      analytical: { 
        1: 'What data defines this context?', 
        2: 'Quantify each observation where possible', 
        3: 'What statistical patterns emerge?', 
        4: 'Where do the numbers conflict?', 
        5: 'What does the data definitively tell us?' 
      },
      first_principles: { 
        1: 'Strip away assumptionsâ€”what remains?', 
        2: 'What are the fundamental observations?', 
        3: 'What patterns exist at the most basic level?', 
        4: 'What constraints are real vs assumed?', 
        5: 'Build up: what must be true?' 
      },
      strategic: { 
        1: 'What is the competitive/strategic landscape?', 
        2: 'What signals indicate threats or opportunities?', 
        3: 'What patterns suggest strategic positioning?', 
        4: 'Where are strategic trade-offs in tension?', 
        5: 'What strategic direction emerges?' 
      },
      futures: { 
        1: 'What future is this context moving toward?', 
        2: 'What weak signals do you observe?', 
        3: 'What patterns suggest emerging trends?', 
        4: 'What alternative futures are in tension?', 
        5: 'What future scenario feels most probable?' 
      },
      scientific: { 
        1: 'What is the hypothesis space?', 
        2: 'What empirical observations exist?', 
        3: 'What patterns suggest causation vs correlation?', 
        4: 'What evidence contradicts the hypothesis?', 
        5: 'What testable insight emerges?' 
      },
      economic: { 
        1: 'What is the economic context?', 
        2: 'What incentives and costs do you observe?', 
        3: 'What patterns of value exchange exist?', 
        4: 'Where do economic interests conflict?', 
        5: 'What economic truth emerges?' 
      },
      ethical: { 
        1: 'Who are the stakeholders affected?', 
        2: 'What impactsâ€”positive and negativeâ€”exist?', 
        3: 'What patterns of fairness or harm emerge?', 
        4: 'What ethical principles are in tension?', 
        5: 'What is the right thing to do?' 
      },
      narrative: { 
        1: 'What is the story being told?', 
        2: 'What characters and motivations do you see?', 
        3: 'What narrative patterns are emerging?', 
        4: 'Where does the story have conflict?', 
        5: 'What is the moral of this story?' 
      },
    };
    return hints[lens]?.[step.stepNumber];
  };
  
  // Export functions
  const generateExportContent = (format: 'markdown' | 'html') => {
    const lines = [`# Insight Stack: ${workspace?.title || 'Session'}`, '', `**Thinking Lens:** ${session?.thinkingLens || 'Automatic'}`, `**Date:** ${new Date().toLocaleDateString()}`, ''];
    toolkit?.steps.forEach((step) => { lines.push(`## Step ${step.stepNumber}: ${step.label}`, '', stepResponses[step.stepNumber] || '(Not filled)', ''); });
    lines.push('## Insights', '');
    allInsights.forEach((insight, i) => { lines.push(`${i + 1}. ${insight}`); });
    lines.push('', '## Sentence of Truth', '', manualSentence || aiContent.sentenceOfTruth || '(Not yet defined)', '');
    lines.push('## Necessary Moves', '');
    aiContent.necessaryMoves.forEach((move, i) => { lines.push(`${i + 1}. ${move}`); });
    return lines.join('\n');
  };
  
  const handleCopy = async () => {
    await navigator.clipboard.writeText(generateExportContent('markdown'));
    setExportStatus('Copied!'); setTimeout(() => setExportStatus(null), 2000);
  };
  const handleDownloadMarkdown = () => {
    const blob = new Blob([generateExportContent('markdown')], { type: 'text/markdown' });
    const url = URL.createObjectURL(blob); const a = document.createElement('a'); a.href = url;
    a.download = `insight-stack-${workspace?.title?.toLowerCase().replace(/\s+/g, '-') || 'session'}.md`;
    a.click(); URL.revokeObjectURL(url);
    setExportStatus('Downloaded!'); setTimeout(() => setExportStatus(null), 2000);
  };
  const handleExportPDF = () => {
    const w = window.open('', '_blank'); if (w) { w.document.write(generateExportContent('html')); w.document.close(); w.print(); }
    setExportStatus('Print dialog opened'); setTimeout(() => setExportStatus(null), 2000);
  };
  const handleEmail = () => {
    if (!emailAddress) { setExportStatus('Enter email'); setTimeout(() => setExportStatus(null), 2000); return; }
    window.location.href = `mailto:${emailAddress}?subject=${encodeURIComponent(`Insight Stack: ${workspace?.title || 'Session'}`)}&body=${encodeURIComponent(generateExportContent('markdown'))}`;
    setExportStatus('Opening email...'); setTimeout(() => { setExportStatus(null); setEmailAddress(''); }, 2000);
  };
  
  const allInsights = [...aiContent.insights, ...manualInsights.filter(i => i.trim())];
  
  if (!session || !toolkit) return <div className="flex items-center justify-center h-96"><p className="text-fresco-graphite-light">Session not found</p></div>;
  
  return (
    <div className="flex h-full bg-fresco-white">
      {/* Main Column */}
      <div className="flex-1 overflow-y-auto">
        <div className="max-w-[720px] mx-auto px-8 py-10">
          {/* Header */}
          <div className="mb-10">
            <div className="flex items-center justify-between mb-8">
              <button type="button" onClick={() => onBack?.()} className="flex items-center gap-2 text-fresco-sm text-fresco-graphite-mid hover:text-fresco-black transition-colors">
                <ChevronLeft className="w-4 h-4" /><span>Back to {workspace?.title || 'Workspace'}</span>
              </button>
              <ThinkingLensSelector value={session.thinkingLens} onChange={handleLensChange} recommendedModes={toolkit.primaryModes} />
            </div>
            
            <div className="flex items-center gap-2 mb-3">
              <img src="/01-investigate.png" alt="Investigate" className="w-4 h-4 opacity-60" />
              <span className="fresco-label">Investigate</span>
            </div>
            <h1 className="text-fresco-3xl font-medium text-fresco-black tracking-tight mb-3">{toolkit.name}</h1>
            <p className="text-fresco-base text-fresco-graphite-mid">{toolkit.subtitle}</p>
          </div>
          
          {/* Steps */}
          <div className="space-y-10">
            {toolkit.steps.map((step, index) => (
              <motion.div key={step.stepNumber} initial={{ opacity: 0, y: 20 }} animate={{ opacity: 1, y: 0 }} transition={{ delay: index * 0.05 }}>
                <div className="fresco-step-label">Step {step.stepNumber} â€” {step.label}</div>
                <p className="text-fresco-lg text-fresco-black mb-4">{step.prompt}</p>
                
                {/* Input area with voice and file buttons */}
                <div className="relative">
                  <textarea
                    value={stepResponses[step.stepNumber] || ''}
                    onChange={(e) => handleStepChange(step.stepNumber, e.target.value)}
                    placeholder={step.placeholder}
                    className="fresco-input-lg pr-24"
                    style={{ minHeight: step.minHeight || 120 }}
                  />
                  
                  {/* Input mode buttons */}
                  <div className="absolute bottom-3 right-3 flex items-center gap-2">
                    {/* Voice button */}
                    <button
                      onClick={() => isRecording === step.stepNumber ? stopRecording() : startRecording(step.stepNumber)}
                      className={cn(
                        "p-2 rounded-full transition-all",
                        isRecording === step.stepNumber 
                          ? "bg-fresco-graphite text-white animate-pulse" 
                          : "bg-fresco-light-gray text-fresco-graphite-mid hover:bg-fresco-border hover:text-fresco-black"
                      )}
                      title={isRecording === step.stepNumber ? "Stop recording" : "Record voice"}
                    >
                      {isRecording === step.stepNumber ? <MicOff className="w-4 h-4" /> : <Mic className="w-4 h-4" />}
                    </button>
                    
                    {/* File upload button */}
                    <button
                      onClick={() => fileInputRefs.current[step.stepNumber]?.click()}
                      className="p-2 rounded-full bg-fresco-light-gray text-fresco-graphite-mid hover:bg-fresco-border hover:text-fresco-black transition-all"
                      title="Upload file"
                    >
                      <Upload className="w-4 h-4" />
                    </button>
                    <input
                      ref={el => fileInputRefs.current[step.stepNumber] = el}
                      type="file"
                      multiple
                      accept=".txt,.md,.csv,.json,.pdf,image/*"
                      className="hidden"
                      onChange={(e) => handleFileUpload(step.stepNumber, e.target.files)}
                    />
                  </div>
                </div>
                
                {/* Recording indicator */}
                {isRecording === step.stepNumber && (
                  <div className="mt-2 flex items-center gap-2 text-fresco-graphite text-fresco-sm">
                    <span className="w-2 h-2 rounded-full bg-fresco-graphite animate-pulse" />
                    Recording... {formatRecordingTime(recordingTime)}
                  </div>
                )}
                
                {/* Uploaded files */}
                {stepFiles[step.stepNumber]?.length > 0 && (
                  <div className="mt-3 flex flex-wrap gap-2">
                    {stepFiles[step.stepNumber].map((file) => (
                      <div key={file.name} className="flex items-center gap-2 px-3 py-1.5 bg-fresco-light-gray rounded-full text-fresco-sm">
                        {file.type.startsWith('image/') ? <Image className="w-3.5 h-3.5" /> : <FileType className="w-3.5 h-3.5" />}
                        <span className="max-w-[150px] truncate">{file.name}</span>
                        <button onClick={() => removeFile(step.stepNumber, file.name)} className="text-fresco-graphite-light hover:text-red-500 transition-colors">
                          <X className="w-3.5 h-3.5" />
                        </button>
                      </div>
                    ))}
                  </div>
                )}
                
                {getLensHint(step) && <p className="mt-3 text-fresco-sm text-fresco-graphite-light italic">ðŸ’¡ {getLensHint(step)}</p>}
              </motion.div>
            ))}
          </div>
          
          <div className="mt-16 pt-8 border-t border-fresco-border-light">
            <p className="text-fresco-xs text-fresco-graphite-light">Saved {session.updatedAt ? formatRelativeTime(session.updatedAt) : 'just now'}</p>
          </div>
        </div>
      </div>
      
      {/* Output Panel */}
      <div className="w-[340px] border-l border-fresco-border-light bg-fresco-off-white overflow-y-auto">
        <div className="p-6">
          <div className="flex items-center justify-between mb-6">
            <h2 className="text-fresco-lg font-medium text-fresco-black">Output</h2>
            {isGenerating && <div className="flex items-center gap-2 text-fresco-sm text-fresco-graphite-light"><Loader2 className="w-4 h-4 animate-spin" /><span>Thinking...</span></div>}
          </div>
          
          {/* Insights */}
          <div className="mb-8">
            <div className="flex items-center justify-between mb-4">
              <span className="fresco-label">Insights</span>
              <div className="flex items-center gap-1">
                <button onClick={generateContent} disabled={isGenerating} className="p-1.5 text-fresco-graphite-light hover:text-fresco-black rounded transition-colors disabled:opacity-50">
                  <RefreshCw className={cn("w-4 h-4", isGenerating && "animate-spin")} />
                </button>
                <button onClick={handleAddManualInsight} className="p-1.5 text-fresco-graphite-light hover:text-fresco-black rounded transition-colors">
                  <Plus className="w-4 h-4" />
                </button>
              </div>
            </div>
            
            {allInsights.length === 0 && !isGenerating ? (
              <div className="py-6 text-center">
                <p className="text-fresco-sm text-fresco-graphite-light mb-4">Fill in the steps to generate insights...</p>
                <button onClick={generateContent} disabled={isGenerating} className="fresco-btn-sm fresco-btn">
                  <Sparkles className="w-3.5 h-3.5" /><span>Generate</span>
                </button>
              </div>
            ) : (
              <div className="space-y-3">
                {aiContent.insights.map((insight, i) => (
                  <div key={`ai-${i}`} className="p-3 bg-fresco-light-gray rounded-fresco text-fresco-sm text-fresco-graphite-soft">{insight}</div>
                ))}
                {manualInsights.map((insight, i) => (
                  <div key={`manual-${i}`} className="relative group">
                    <input type="text" value={insight} onChange={(e) => handleManualInsightChange(i, e.target.value)} placeholder="Add your insight..."
                      className="w-full p-3 bg-fresco-light-gray rounded-fresco text-fresco-sm text-fresco-graphite-soft border-none focus:ring-1 focus:ring-fresco-border outline-none" />
                    <button onClick={() => handleRemoveManualInsight(i)} className="absolute right-2 top-1/2 -translate-y-1/2 opacity-0 group-hover:opacity-100 p-1 text-fresco-graphite-light hover:text-red-500 transition-colors">
                      <Trash2 className="w-3.5 h-3.5" />
                    </button>
                  </div>
                ))}
              </div>
            )}
          </div>
          
          {/* Sentence of Truth */}
          <div className="mb-8">
            <span className="fresco-label block mb-4">Sentence of Truth</span>
            <SentenceOfTruth
              content={manualSentence || aiContent.sentenceOfTruth}
              isLocked={session.sentenceOfTruth?.isLocked || false}
              onChange={handleSentenceChange}
              onToggleLock={handleToggleLock}
              suggestions={aiContent.sentenceOfTruth && aiContent.sentenceOfTruth !== manualSentence ? [aiContent.sentenceOfTruth] : []}
              onUseSuggestion={handleUseSuggestion}
            />
          </div>
          
          {/* Necessary Moves */}
          <div className="mb-8">
            <span className="fresco-label block mb-4">Necessary Moves</span>
            {aiContent.necessaryMoves.length === 0 ? (
              <p className="text-fresco-sm text-fresco-graphite-light">Generate insights to see suggested next moves...</p>
            ) : (
              <div className="space-y-2">
                {aiContent.necessaryMoves.map((move, i) => (
                  <div key={i} className="flex items-start gap-3 p-3 bg-fresco-light-gray rounded-fresco">
                    <div className="w-5 h-5 rounded-full border border-fresco-border flex items-center justify-center flex-shrink-0 mt-0.5">
                      <span className="text-fresco-xs text-fresco-graphite-light">{i + 1}</span>
                    </div>
                    <p className="text-fresco-sm text-fresco-graphite-soft">{move}</p>
                  </div>
                ))}
              </div>
            )}
          </div>
          
          {/* Export Session Button */}
          <div className="pt-6 border-t border-fresco-border-light">
            <button 
              onClick={() => setShowExportModal(true)} 
              className="fresco-btn w-full"
            >
              <Download className="w-4 h-4" />
              <span>Export Session</span>
            </button>
          </div>
        </div>
      </div>
      
      {/* Export Modal */}
      <AnimatePresence>
        {showExportModal && (
          <motion.div initial={{ opacity: 0 }} animate={{ opacity: 1 }} exit={{ opacity: 0 }} className="fixed inset-0 bg-black/40 flex items-center justify-center z-50" onClick={() => setShowExportModal(false)}>
            <motion.div initial={{ scale: 0.95, opacity: 0 }} animate={{ scale: 1, opacity: 1 }} exit={{ scale: 0.95, opacity: 0 }} onClick={(e) => e.stopPropagation()}
              className="bg-white dark:bg-gray-900 rounded-fresco-lg p-6 max-w-md w-full mx-4 shadow-fresco-lg">
              <div className="flex items-center justify-between mb-6">
                <h3 className="text-fresco-lg font-medium text-fresco-black">Export Session</h3>
                <button onClick={() => setShowExportModal(false)} className="p-1 text-fresco-graphite-light hover:text-fresco-black transition-colors"><X className="w-5 h-5" /></button>
              </div>
              {exportStatus && <div className="mb-4 p-3 bg-fresco-light-gray rounded-fresco text-fresco-sm text-fresco-black flex items-center gap-2"><Check className="w-4 h-4" />{exportStatus}</div>}
              <div className="space-y-3">
                <button onClick={handleCopy} className="w-full flex items-center gap-3 p-4 border border-fresco-border rounded-fresco hover:bg-fresco-light-gray transition-colors">
                  <Copy className="w-5 h-5 text-fresco-graphite-mid" /><div className="text-left"><p className="text-fresco-base text-fresco-black">Copy to Clipboard</p><p className="text-fresco-sm text-fresco-graphite-light">Copy as formatted text</p></div>
                </button>
                <button onClick={handleDownloadMarkdown} className="w-full flex items-center gap-3 p-4 border border-fresco-border rounded-fresco hover:bg-fresco-light-gray transition-colors">
                  <FileText className="w-5 h-5 text-fresco-graphite-mid" /><div className="text-left"><p className="text-fresco-base text-fresco-black">Download Markdown</p><p className="text-fresco-sm text-fresco-graphite-light">Save as .md file</p></div>
                </button>
                <button onClick={handleExportPDF} className="w-full flex items-center gap-3 p-4 border border-fresco-border rounded-fresco hover:bg-fresco-light-gray transition-colors">
                  <FileDown className="w-5 h-5 text-fresco-graphite-mid" /><div className="text-left"><p className="text-fresco-base text-fresco-black">Export PDF</p><p className="text-fresco-sm text-fresco-graphite-light">Print or save as PDF</p></div>
                </button>
                <div className="p-4 border border-fresco-border rounded-fresco">
                  <div className="flex items-center gap-3 mb-3"><Mail className="w-5 h-5 text-fresco-graphite-mid" /><p className="text-fresco-base text-fresco-black">Share via Email</p></div>
                  <div className="flex gap-2">
                    <input type="email" value={emailAddress} onChange={(e) => setEmailAddress(e.target.value)} placeholder="email@example.com" className="flex-1 px-3 py-2 border border-fresco-border rounded-fresco text-fresco-sm focus:outline-none focus:border-fresco-black" />
                    <button onClick={handleEmail} className="fresco-btn fresco-btn-primary fresco-btn-sm">Send</button>
                  </div>
                </div>
              </div>
            </motion.div>
          </motion.div>
        )}
      </AnimatePresence>
    </div>
  );
}
